{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4925c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276d6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/tusharsingharoy/Downloads/the-verdict.txt\",\"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f088543",
   "metadata": {},
   "source": [
    "## Tokenized the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220d2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--']\n",
      "Text Size :20479\n",
      "Total number of token :8921\n",
      "Total Number of teken without whitespace 4456\n",
      "Total Number of unique token 1181\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "token=re.split(r'([,.:;?\"\\s)]|--|\\s)',raw_text)\n",
    "print(token[:20])   ## Print frist 20 token \n",
    "print(f\"Text Size :{len(raw_text)}\")\n",
    "print(f\"Total number of token :{len(token)}\")\n",
    "token_e_strip = [item.strip() for item in token if item.strip()]   ## item.strip() removes whitespace characters \n",
    "print(f\"Total Number of teken without whitespace {len(token_e_strip)}\")\n",
    "uni_token=list(set(token_e_strip))    ## Store only the unique text\n",
    "print(f\"Total Number of unique token {len(uni_token)}\")\n",
    "uni_token=sorted(uni_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f167a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer,token in enumerate(uni_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba59985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('\"', 0), (\"'\", 1), (\"'Are\", 2), (\"'It's\", 3), (\"'coming'\", 4), (\"'done'\", 5), (\"'subject\", 6), (\"'technique'\", 7), (\"'way\", 8), ('(I', 9), ('(Though', 10), (')', 11), (',', 12), ('--', 13), ('.', 14), (':', 15), (';', 16), ('?', 17), ('A', 18), ('Ah', 19), ('Among', 20), ('And', 21), ('Arrt', 22), ('As', 23), ('At', 24), ('Be', 25), ('Begin', 26), ('Burlington', 27), ('But', 28), ('By', 29), ('Carlo', 30), ('Chicago', 31), ('Claude', 32), ('Come', 33), ('Croft', 34), ('Destroyed', 35), ('Devonshire', 36), (\"Don't\", 37), ('Dubarry_', 38), ('Emperors', 39), ('Florence', 40), ('For', 41), ('Gallery', 42), ('Gideon', 43), ('Gisburn', 44), ('Gisburn!', 45), (\"Gisburn's\", 46), ('Gisburns', 47), ('Grafton', 48), ('Greek', 49), ('Grindle', 50), (\"Grindle's\", 51), ('Grindles', 52), ('HAD', 53), ('Had', 54), ('Hang', 55), ('Has', 56), ('He', 57), ('Her', 58), ('Hermia', 59), (\"Hermia's\", 60), ('His', 61), ('How', 62), ('I', 63), (\"I'd\", 64), (\"I'll\", 65), (\"I've\", 66), ('If', 67), ('In', 68), ('It', 69), (\"It's\", 70), ('Jack', 71), ('Jack!', 72), (\"Jack's\", 73), ('Jove', 74), ('Jove!', 75), ('Just', 76), ('Lord', 77), ('Made', 78), ('Miss', 79), (\"Money's\", 80), ('Monte', 81), ('Moon-dancers', 82), ('Mr', 83), ('Mrs', 84), ('My', 85), ('Never', 86), ('No', 87), ('Now', 88), ('Nutley', 89), ('Of', 90), ('Oh', 91), ('On', 92), ('Once', 93), ('Only', 94), ('Or', 95), ('Perhaps', 96), ('Poor', 97), ('Professional', 98), ('Renaissance', 99), ('Rickham', 100), ('Rickham!', 101), ('Riviera', 102), ('Rome', 103), ('Russian', 104), ('Sevres', 105), ('She', 106), (\"She's\", 107), ('Stroud', 108), ('Stroud!', 109), (\"Stroud's\", 110), ('Strouds', 111), ('Suddenly', 112), ('That', 113), (\"That's\", 114), ('The', 115), ('Then', 116), ('There', 117), ('They', 118), ('This', 119), ('Those', 120), ('Thwing', 121), (\"Thwing's\", 122), ('Thwings', 123), ('To', 124), ('Usually', 125), ('Venetian', 126), ('Victor', 127), ('Was', 128), ('We', 129), ('Well', 130), ('Well!', 131), ('What', 132), ('When', 133), ('Why', 134), ('Yes', 135), ('You', 136), ('_I', 137), ('_am_', 138), ('_famille-verte_', 139), ('_felt_', 140), ('_has_', 141), ('_have_', 142), ('_jardiniere_', 143), ('_mine_', 144), ('_not_', 145), ('_rose', 146), ('_rs_', 147), ('_that_', 148), ('_the_', 149), ('_was_', 150), ('_were_', 151), ('a', 152), ('abdication', 153), ('able', 154), ('about', 155), ('above', 156), ('abruptly', 157), ('absolute', 158), ('absorbed', 159), ('absurdity', 160), ('academic', 161), ('accuse', 162), ('accustomed', 163), ('across', 164), ('activity', 165), ('add', 166), ('added', 167), ('admirers', 168), ('adopted', 169), ('adulation', 170), ('advance', 171), ('aesthetic', 172), ('affect', 173), ('afraid', 174), ('after', 175), ('afterward', 176), ('again', 177), ('ago', 178), ('ah', 179), ('air', 180), ('alive', 181), ('all', 182), ('almost', 183), ('alone', 184), ('along', 185), ('always', 186), ('amazement', 187), ('amid', 188), ('among', 189), ('amplest', 190), ('amusing', 191), ('an', 192), ('and', 193), ('another', 194), ('answer', 195), ('answered', 196), ('any', 197), ('anything', 198), ('anywhere', 199), ('apparent', 200), ('apparently', 201), ('appearance', 202), ('appeared', 203), ('appointed', 204), ('are', 205), ('arm', 206), ('arm-chair', 207), ('arm-chairs', 208), ('arms', 209), ('art', 210), ('articles', 211), ('artist', 212), ('as', 213), ('aside', 214), ('asked', 215), ('at', 216), ('atmosphere', 217), ('atom', 218), ('attack', 219), ('attention', 220), ('attitude', 221), ('audacities', 222), ('away', 223), ('awful', 224), ('axioms', 225), ('azaleas', 226), ('back', 227), ('background', 228), ('balance', 229), ('balancing', 230), ('balustraded', 231), ('basking', 232), ('bath-rooms', 233), ('be', 234), ('beaming', 235), ('bean-stalk', 236), ('bear', 237), ('beard', 238), ('beauty', 239), ('became', 240), ('because', 241), ('becoming', 242), ('bed', 243), ('been', 244), ('before', 245), ('began', 246), ('begun', 247), ('behind', 248), ('being', 249), ('believed', 250), ('beneath', 251), ('bespoke', 252), ('better', 253), ('between', 254), ('big', 255), ('bits', 256), ('bitterness', 257), ('blocked', 258), ('born', 259), ('borne', 260), ('boudoir', 261), ('bravura', 262), ('break', 263), ('breaking', 264), ('breathing', 265), ('bric-a-brac', 266), ('briefly', 267), ('brings', 268), ('bronzes', 269), ('brought', 270), ('brown', 271), ('brush', 272), ('bull', 273), ('business', 274), ('but', 275), ('buying', 276), ('by', 277), ('called', 278), ('came', 279), ('can', 280), ('canvas', 281), ('canvases', 282), ('cards', 283), ('care', 284), ('career', 285), ('caught', 286), ('central', 287), ('chair', 288), ('chap', 289), ('characteristic', 290), ('charming', 291), ('cheap', 292), ('check', 293), ('cheeks', 294), ('chest', 295), ('chimney-piece', 296), ('chucked', 297), ('cigar', 298), ('cigarette', 299), ('cigars', 300), ('circulation', 301), ('circumstance', 302), (\"circus-clown's\", 303), ('claimed', 304), ('clasping', 305), ('clear', 306), ('cleverer', 307), ('close', 308), ('clue', 309), ('coat', 310), ('collapsed', 311), ('colour', 312), ('come', 313), ('comfortable', 314), ('coming', 315), ('companion', 316), ('compared', 317), ('complex', 318), ('confident', 319), ('congesting', 320), ('conjugal', 321), ('constraint', 322), ('consummate', 323), ('contended', 324), ('continued', 325), ('corner', 326), ('corrected', 327), ('could', 328), (\"couldn't\", 329), ('count', 330), ('countenance', 331), ('couple', 332), ('course', 333), ('covered', 334), ('craft', 335), ('cried', 336), ('crossed', 337), ('crowned', 338), ('crumbled', 339), ('cry', 340), ('cured', 341), ('curiosity', 342), ('curious', 343), ('current', 344), ('curtains', 345), ('dabble', 346), ('damask', 347), ('dark', 348), ('dashed', 349), ('day', 350), ('days', 351), ('dead', 352), ('deadening', 353), ('dear', 354), ('deep', 355), (\"deerhound's\", 356), ('degree', 357), ('delicate', 358), ('demand', 359), ('denied', 360), ('deploring', 361), ('deprecating', 362), ('deprecatingly', 363), ('desire', 364), ('destroyed', 365), ('destruction', 366), ('desultory', 367), ('detail', 368), ('diagnosis', 369), ('did', 370), (\"didn't\", 371), ('died', 372), ('dim', 373), ('dimmest', 374), ('dingy', 375), ('dining-room', 376), ('disarming', 377), ('discovery', 378), ('discrimination', 379), ('discussion', 380), ('disdain', 381), ('disdained', 382), ('disease', 383), ('disguised', 384), ('display', 385), ('dissatisfied', 386), ('distinguished', 387), ('distract', 388), ('divert', 389), ('do', 390), (\"doesn't\", 391), ('doing', 392), ('domestic', 393), (\"don't\", 394), ('done', 395), ('donkey', 396), ('down', 397), ('dozen', 398), ('dragged', 399), ('drawing-room', 400), ('drawing-rooms', 401), ('drawn', 402), ('dress-closets', 403), ('drew', 404), ('dropped', 405), ('each', 406), ('earth', 407), ('ease', 408), ('easel', 409), ('easy', 410), ('echoed', 411), ('economy', 412), ('effect', 413), ('effects', 414), ('efforts', 415), ('egregious', 416), ('eighteenth-century', 417), ('elbow', 418), ('elegant', 419), ('else', 420), ('embarrassed', 421), ('enabled', 422), ('end', 423), ('endless', 424), ('enjoy', 425), ('enlightenment', 426), ('enough', 427), ('ensuing', 428), ('equally', 429), ('equanimity', 430), ('escape', 431), ('established', 432), ('etching', 433), ('even', 434), ('event', 435), ('ever', 436), ('everlasting', 437), ('every', 438), ('exasperated', 439), ('except', 440), ('excuse', 441), ('excusing', 442), ('existed', 443), ('expected', 444), ('exquisite', 445), ('exquisitely', 446), ('extenuation', 447), ('exterminating', 448), ('extracting', 449), ('eye', 450), ('eyebrows', 451), ('eyes', 452), ('face', 453), ('faces', 454), ('fact', 455), ('faded', 456), ('failed', 457), ('failure', 458), ('fair', 459), ('faith', 460), ('false', 461), ('familiar', 462), ('fancy', 463), ('fashionable', 464), ('fate', 465), ('feather', 466), ('feet', 467), ('fell', 468), ('fellow', 469), ('felt', 470), ('few', 471), ('fewer', 472), ('finality', 473), ('find', 474), ('fingers', 475), ('first', 476), ('fit', 477), ('fitting', 478), ('five', 479), ('flash', 480), ('flashed', 481), ('florid', 482), ('flowers', 483), ('fluently', 484), ('flung', 485), ('follow', 486), ('followed', 487), ('fond', 488), ('footstep', 489), ('for', 490), ('forced', 491), ('forcing', 492), ('forehead', 493), ('foreign', 494), ('foreseen', 495), ('forgive', 496), ('forgotten', 497), ('form', 498), ('formed', 499), ('forming', 500), ('forward', 501), ('fostered', 502), ('found', 503), ('foundations', 504), ('fragment', 505), ('fragments', 506), ('frame', 507), ('frames', 508), ('frequently', 509), (\"friend's\", 510), ('from', 511), ('full', 512), ('fullest', 513), ('furiously', 514), ('furrowed', 515), ('garlanded', 516), ('garlands', 517), ('gave', 518), ('genial', 519), ('genius', 520), ('gesture', 521), ('get', 522), ('getting', 523), ('give', 524), ('given', 525), ('glad', 526), ('glanced', 527), ('glimpse', 528), ('gloried', 529), ('glory', 530), ('go', 531), ('going', 532), ('gone', 533), ('good', 534), ('good-breeding', 535), ('good-humoured', 536), ('got', 537), ('grace', 538), ('gradually', 539), ('gray', 540), ('grayish', 541), ('great', 542), ('greatest', 543), ('greatness', 544), ('grew', 545), ('groping', 546), ('growing', 547), ('had', 548), (\"hadn't\", 549), ('hair', 550), ('half', 551), ('half-light', 552), ('half-mechanically', 553), ('hall', 554), ('hand', 555), ('hands', 556), ('handsome', 557), ('hanging', 558), ('happen', 559), ('happened', 560), ('hard', 561), ('hardly', 562), ('have', 563), (\"haven't\", 564), ('having', 565), ('he', 566), (\"he'd\", 567), (\"he's\", 568), ('head', 569), ('hear', 570), ('heard', 571), ('heart', 572), ('height', 573), ('her', 574), ('here', 575), ('hermit', 576), ('herself', 577), ('hesitations', 578), ('hide', 579), ('high', 580), ('him', 581), ('him!', 582), ('himself', 583), ('hint', 584), ('his', 585), ('his!', 586), ('history', 587), ('holding', 588), ('home', 589), ('honour', 590), ('hooded', 591), ('hostess', 592), ('hot-house', 593), ('hour', 594), ('hours', 595), ('house', 596), ('how', 597), (\"how'\", 598), ('hung', 599), ('husband', 600), (\"husband's\", 601), ('idea', 602), ('idle', 603), ('idling', 604), ('if', 605), ('immediately', 606), ('in', 607), ('incense', 608), ('indifferent', 609), ('inevitable', 610), ('inevitably', 611), ('inflexible', 612), ('insensible', 613), ('insignificant', 614), ('instinctively', 615), ('instructive', 616), ('interesting', 617), ('into', 618), ('ironic', 619), ('irony', 620), ('irrelevance', 621), ('irrevocable', 622), ('is', 623), ('it', 624), (\"it's\", 625), ('its', 626), ('itself', 627), ('jealousy', 628), ('just', 629), ('keep', 630), ('kept', 631), ('kind', 632), ('knees', 633), ('knew', 634), ('know', 635), ('known_', 636), ('laid', 637), ('lair', 638), ('landing', 639), ('language', 640), ('last', 641), ('late', 642), ('later', 643), ('latter', 644), (\"latter's\", 645), ('laugh', 646), ('laughed', 647), ('lay', 648), ('leading', 649), ('lean', 650), ('learned', 651), ('least', 652), ('leathery', 653), ('leave', 654), ('led', 655), ('left', 656), ('leisure!', 657), ('lends', 658), ('lent', 659), ('let', 660), ('lies!', 661), ('life', 662), ('life-likeness', 663), ('lift', 664), ('lifted', 665), ('light', 666), ('lightly', 667), ('like', 668), ('liked', 669), ('line', 670), ('lines', 671), ('lingered', 672), ('lips', 673), ('lit', 674), ('little', 675), ('live', 676), ('loathing', 677), ('long', 678), ('longed', 679), ('longer', 680), ('look', 681), ('looked', 682), ('looking', 683), ('lose', 684), ('loss', 685), ('lounging', 686), ('lovely', 687), ('lucky', 688), ('lump', 689), ('luncheon-table', 690), ('luxury', 691), ('lying', 692), ('made', 693), ('make', 694), ('man', 695), ('manage', 696), ('managed', 697), ('mantel-piece', 698), ('marble', 699), ('married', 700), ('may', 701), ('me', 702), ('me!', 703), ('meant', 704), ('mediocrity', 705), ('medium', 706), ('mentioned', 707), ('mere', 708), ('merely', 709), ('met', 710), ('might', 711), ('mighty', 712), (\"millionaire's\", 713), ('mine', 714), ('minute', 715), ('minutes', 716), ('mirrors', 717), ('modest', 718), ('modesty', 719), ('moment', 720), ('money', 721), ('monumental', 722), ('mood', 723), ('morbidly', 724), ('more', 725), ('most', 726), ('mourn', 727), ('mourned', 728), ('moustache', 729), ('moved', 730), ('much', 731), ('muddling', 732), ('multiplied', 733), ('murmur', 734), ('muscles', 735), ('must', 736), ('my', 737), ('myself', 738), ('mysterious', 739), ('naive', 740), ('near', 741), ('nearly', 742), ('negatived', 743), ('nervous', 744), ('nervousness', 745), ('neutral', 746), ('never', 747), ('next', 748), ('no', 749), ('none', 750), ('not', 751), ('note', 752), ('note!', 753), ('nothing', 754), ('now', 755), ('nymphs', 756), ('oak', 757), ('obituary', 758), ('object', 759), ('objects', 760), ('occurred', 761), ('oddly', 762), ('of', 763), ('off', 764), ('often', 765), ('oh', 766), ('old', 767), ('on', 768), ('once', 769), ('one', 770), ('ones', 771), ('only', 772), ('onto', 773), ('open', 774), ('or', 775), ('other', 776), ('our', 777), ('ourselves', 778), ('out', 779), ('outline', 780), ('oval', 781), ('over', 782), ('over!', 783), ('own', 784), ('packed', 785), ('paid', 786), ('paint', 787), ('painted', 788), ('painter', 789), ('painting', 790), ('pale', 791), ('paled', 792), ('palm-trees', 793), ('panel', 794), ('panelling', 795), ('pardonable', 796), ('pardoned', 797), ('part', 798), ('passages', 799), ('passing', 800), ('past', 801), ('past!', 802), ('pastels', 803), ('pathos', 804), ('patient', 805), ('people', 806), ('perceptible', 807), ('perfect', 808), ('persistence', 809), ('persuasively', 810), ('phrase', 811), ('picture', 812), ('pictures', 813), ('pines', 814), ('pink', 815), ('place', 816), ('placed', 817), ('plain', 818), ('platitudes', 819), ('pleased', 820), ('pockets', 821), ('point', 822), ('poised', 823), ('poor', 824), ('portrait', 825), ('posing', 826), ('possessed', 827), ('poverty', 828), ('predicted', 829), ('preliminary', 830), ('presenting', 831), ('prestidigitation', 832), ('pretty', 833), ('previous', 834), ('price', 835), ('pride', 836), ('princely', 837), ('prism', 838), ('problem', 839), ('proclaiming', 840), ('prodigious', 841), ('profusion', 842), ('protest', 843), ('prove', 844), ('public', 845), ('purblind', 846), ('purely', 847), ('pushed', 848), ('put', 849), ('qualities', 850), ('quality', 851), ('queerly', 852), ('question', 853), ('quickly', 854), ('quietly', 855), ('quite', 856), ('quote', 857), ('rain', 858), ('raised', 859), ('random', 860), ('rather', 861), ('real', 862), ('really', 863), ('reared', 864), ('reason', 865), ('reassurance', 866), ('recovering', 867), ('recreated', 868), ('reflected', 869), ('reflection', 870), ('regrets', 871), ('relatively', 872), ('remained', 873), ('remember', 874), ('reminded', 875), ('repeating', 876), ('represented', 877), ('reproduction', 878), ('resented', 879), ('resolve', 880), ('resources', 881), ('rest', 882), ('rich', 883), ('ridiculous', 884), ('robbed', 885), ('romantic!', 886), ('room', 887), ('rose', 888), ('rule', 889), ('run', 890), ('said', 891), ('same', 892), ('satisfaction', 893), ('savour', 894), ('saw', 895), ('say', 896), ('saying', 897), ('says', 898), ('scorn', 899), ('scornful', 900), ('secret', 901), ('see', 902), ('seemed', 903), ('seen', 904), ('self-confident', 905), ('send', 906), ('sensation', 907), ('sensitive', 908), ('sent', 909), ('serious', 910), ('set', 911), ('sex', 912), ('shade', 913), ('shaking', 914), ('shall', 915), ('she', 916), (\"she's\", 917), ('shirked', 918), ('short', 919), ('should', 920), ('shoulder', 921), ('shoulders', 922), ('show', 923), ('showed', 924), ('showy', 925), ('showy!', 926), ('shrug', 927), ('shrugged', 928), ('sight', 929), ('sign', 930), ('silent', 931), ('silver', 932), ('similar', 933), ('simpleton', 934), ('simplifications', 935), ('simply', 936), ('since', 937), ('single', 938), ('sitter', 939), ('sitters', 940), ('sketch', 941), ('skill', 942), ('slight', 943), ('slightly', 944), ('slowly', 945), ('small', 946), ('smile', 947), ('smiling', 948), ('sneer', 949), ('so', 950), ('solace', 951), ('some', 952), ('somebody', 953), ('something', 954), ('spacious', 955), ('spaniel', 956), ('speaking-tubes', 957), ('speculations', 958), ('spite', 959), ('splash', 960), ('square', 961), ('stairs', 962), ('stammer', 963), ('stand', 964), ('standing', 965), ('started', 966), ('stay', 967), ('stay!', 968), ('still', 969), ('stocked', 970), ('stood', 971), ('stopped', 972), ('stopping', 973), ('straddling', 974), ('straight', 975), ('strain', 976), ('straining', 977), ('strange', 978), ('straw', 979), ('stream', 980), ('stroke', 981), ('strokes', 982), ('strolled', 983), ('strongest', 984), ('strongly', 985), ('struck', 986), ('studio', 987), ('stuff', 988), ('subject', 989), ('substantial', 990), ('suburban', 991), ('such', 992), ('suddenly', 993), ('suffered', 994), ('sugar', 995), ('suggested', 996), ('sunburn', 997), ('sunburnt', 998), ('sunlit', 999), ('superb', 1000), ('sure', 1001), ('surest', 1002), ('surface', 1003), ('surprise', 1004), ('surprised', 1005), ('surrounded', 1006), ('suspected', 1007), ('sweetly', 1008), ('sweetness', 1009), ('swelling', 1010), ('swept', 1011), ('swum', 1012), ('table', 1013), ('take', 1014), ('taken', 1015), ('talking', 1016), ('tea', 1017), ('tears', 1018), ('technicalities', 1019), ('tell', 1020), ('tells', 1021), ('tempting', 1022), ('terra-cotta', 1023), ('terrace', 1024), ('terraces', 1025), ('terribly', 1026), ('than', 1027), ('that', 1028), ('the', 1029), ('their', 1030), ('them', 1031), ('then', 1032), ('there', 1033), (\"there's\", 1034), ('therefore', 1035), ('they', 1036), (\"they're\", 1037), ('thin', 1038), ('thing', 1039), ('things', 1040), ('think', 1041), ('this', 1042), ('thither', 1043), ('those', 1044), ('though', 1045), ('thought', 1046), ('three', 1047), ('threshold', 1048), ('threw', 1049), ('through', 1050), ('throwing', 1051), ('tie', 1052), ('till', 1053), ('time', 1054), ('timorously', 1055), ('tinge', 1056), ('tips', 1057), ('tired', 1058), ('to', 1059), ('told', 1060), ('tone', 1061), ('tones', 1062), ('too', 1063), ('took', 1064), ('tottering', 1065), ('touched', 1066), ('toward', 1067), ('trace', 1068), ('trade', 1069), ('transmute', 1070), ('traps', 1071), ('travelled', 1072), ('tribute', 1073), ('tributes', 1074), ('tricks', 1075), ('tried', 1076), ('trouser-presses', 1077), ('true', 1078), ('truth', 1079), ('turned', 1080), ('twenty', 1081), ('twenty-four', 1082), ('twice', 1083), ('twirling', 1084), ('unaccountable', 1085), ('uncertain', 1086), ('under', 1087), ('underlay', 1088), ('underneath', 1089), ('understand', 1090), ('unexpected', 1091), ('untouched', 1092), ('unusual', 1093), ('up', 1094), ('up-stream', 1095), ('upon', 1096), ('upset', 1097), ('upstairs', 1098), ('us', 1099), ('us!', 1100), ('used', 1101), ('usual', 1102), ('value', 1103), ('varnishing', 1104), ('vases', 1105), ('veins', 1106), ('velveteen', 1107), ('very', 1108), ('villa', 1109), ('vindicated', 1110), ('virtuosity', 1111), ('vista', 1112), ('vocation', 1113), ('voice', 1114), ('wall', 1115), ('wander', 1116), ('want', 1117), ('wanted', 1118), ('wants', 1119), ('was', 1120), (\"wasn't\", 1121), ('watched', 1122), ('watching', 1123), ('water-colour', 1124), ('waves', 1125), ('way', 1126), ('weekly', 1127), ('weeks', 1128), (\"weeks'\", 1129), ('welcome', 1130), ('went', 1131), ('were', 1132), ('what', 1133), ('when', 1134), ('whenever', 1135), ('where', 1136), ('which', 1137), ('while', 1138), ('white', 1139), ('white-panelled', 1140), ('who', 1141), ('whole', 1142), ('whom', 1143), ('why', 1144), ('wide', 1145), ('widow', 1146), ('wife', 1147), ('wife!', 1148), (\"wife's\", 1149), ('wild', 1150), ('wincing', 1151), ('window-curtains', 1152), ('wish', 1153), ('with', 1154), ('without', 1155), (\"wits'\", 1156), ('woman', 1157), ('woman!', 1158), ('women', 1159), (\"won't\", 1160), ('wonder', 1161), ('wonder!', 1162), ('wondered', 1163), ('word', 1164), ('work', 1165), ('work!', 1166), ('working', 1167), ('worth', 1168), ('would', 1169), (\"wouldn't\", 1170), ('year', 1171), ('years', 1172), ('yellow', 1173), ('yet', 1174), ('you', 1175), (\"you'd\", 1176), (\"you're\", 1177), ('younger', 1178), ('your', 1179), ('yourself', 1180)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e37343",
   "metadata": {},
   "source": [
    "## Printing frist 20 token with token ID'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "505180a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"', 0)\n",
      "(\"'\", 1)\n",
      "(\"'Are\", 2)\n",
      "(\"'It's\", 3)\n",
      "(\"'coming'\", 4)\n",
      "(\"'done'\", 5)\n",
      "(\"'subject\", 6)\n",
      "(\"'technique'\", 7)\n",
      "(\"'way\", 8)\n",
      "('(I', 9)\n",
      "('(Though', 10)\n",
      "(')', 11)\n",
      "(',', 12)\n",
      "('--', 13)\n",
      "('.', 14)\n",
      "(':', 15)\n",
      "(';', 16)\n",
      "('?', 17)\n",
      "('A', 18)\n",
      "('Ah', 19)\n",
      "('Among', 20)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if(i==20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a81869",
   "metadata": {},
   "source": [
    "## SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff65bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encoder(self,text):\n",
    "        token=re.split(r'([,;:\".}{]?|--|\\s)',text)\n",
    "        token=[item.strip() for item in token if item.strip()]\n",
    "        token_ids=[self.str_to_int[s] for s in token]\n",
    "        return token_ids\n",
    "    \n",
    "    def decoder(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "         # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae929635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verdict_tokenizer = SimpleTokenizer(vocab)\n",
    "# ids=verdict_tokenizer.encoder(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d46b3",
   "metadata": {},
   "source": [
    "## Add <|endoftext|> \n",
    "- Help to join to sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed2282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text=\" <|endoftext|> \".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764914e9",
   "metadata": {},
   "source": [
    "### BYTE PAIR ENCODING (BPE)   [BY using tiktoken]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b61c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken\n",
    "import tiktoken as tik\n",
    "model=[\"gpt2\",\"gpt3\",\"gpt4\"]\n",
    "gpt_vocab=[tik.get_encoding(\"gpt2\"),tik.get_encoding(\"p50k_base\"),tik.get_encoding(\"cl100k_base\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a0851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for gpt2 is: 50257\n",
      "The vocabulary size for gpt3 is: 50281\n",
      "The vocabulary size for gpt4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "for i, (model_name, vocab) in enumerate(zip(model, gpt_vocab)):\n",
    "    print(f\"The vocabulary size for {model_name} is: {vocab.n_vocab}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626c72d",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49164e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5f9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee1ca79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \n",
    "    tokenizer=tik.get_encoding(\"gpt2\")\n",
    "    \n",
    "    dataset=GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ec0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=3, max_length=4, stride=1, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a05bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[  40,  367, 2885, 1464],\n",
      "        [ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619]])\n",
      "Target IDs: tensor([[ 367, 2885, 1464, 1807],\n",
      "        [2885, 1464, 1807, 3619],\n",
      "        [1464, 1807, 3619,  402]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_ids, target_ids in dataloader:\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Target IDs:\", target_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4d5b4",
   "metadata": {},
   "source": [
    "### CREATING TOKEN EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "128c31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size=6\n",
    "output_dim=3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "#####====>  IMP: embedding_layer  is not a list of tensor \n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)  ## Create some random weights \n",
    "print(embedding_layer.weight)    ## PrInt that random weights\n",
    "a=embedding_layer.weight       ## It is a list of tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce382971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))   ## print the 4th row of the embedding weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eacebd",
   "metadata": {},
   "source": [
    "### POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb512424",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let craete the embedding as like GPT 2\n",
    "vocab_size=50257\n",
    "output_dim=256\n",
    "\n",
    "token_embedding_layer=torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2fc69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.1338,  1.0524, -0.3885,  ...,  0.2461,  1.2119,  0.3171],\n",
      "        [ 1.2277, -0.4297, -2.2121,  ..., -0.1640, -0.3348, -0.0221],\n",
      "        [ 1.3382,  0.2706,  0.5071,  ...,  0.0175, -2.1517,  0.3924],\n",
      "        ...,\n",
      "        [-1.4889, -1.2456,  1.8034,  ..., -0.6392, -1.4939,  0.3614],\n",
      "        [-1.0703,  0.2795, -0.2637,  ..., -0.2810, -1.4755, -0.1183],\n",
      "        [-0.0071,  0.4982, -0.3319,  ...,  0.4970,  0.9365, -0.2091]],\n",
      "       requires_grad=True)\n",
      "torch.Size([50257, 256])\n"
     ]
    }
   ],
   "source": [
    "print(token_embedding_layer.weight)\n",
    "print(token_embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4ec4a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "dataloader=dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4,\n",
    "    stride=4, shuffle=False)\n",
    "count=0\n",
    "for input_id,target_id in dataloader:\n",
    "    # print(f\"Input ID : {input_id}\")\n",
    "    # print(f\"Input ID : {target_id}\")\n",
    "    count=count+1\n",
    "    \n",
    "\n",
    "print(count)  ## len(token_ids) = num_samples × stride + max_length = 1280 × 4 + 4 = 5124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc2ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ID : tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input ID : tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "for input_id,target_id in dataloader:\n",
    "    print(f\"Input ID : {input_id}\")\n",
    "    print(f\"Input ID : {target_id}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9de8b4",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97633397",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 4\n",
    "output_dim=256\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f369d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6303, -0.4848, -0.1366,  ...,  1.0345, -0.5012,  1.1045],\n",
      "        [ 0.2062,  0.6078,  0.7187,  ..., -0.4628, -0.2319,  1.1980],\n",
      "        [ 0.5806, -1.3846,  0.3266,  ...,  0.8579,  0.5059,  1.0243],\n",
      "        [ 1.4323,  0.2217,  0.8599,  ...,  0.4827,  0.8459,  1.3038]],\n",
      "       requires_grad=True)\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(pos_embedding_layer.weight)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(4))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80dd1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0a74696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of token Embedding  torch.Size([8, 4, 256])\n",
      "Shape of pos Embedding  torch.Size([4, 256])\n",
      "Shape of input Embedding  torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(f\"Shape of token Embedding  {token_embeddings.shape}\")\n",
    "print(f\"Shape of pos Embedding  {pos_embeddings.shape}\")   ## Braodcast the row\n",
    "print(f\"Shape of input Embedding  {input_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a011f",
   "metadata": {},
   "source": [
    "##  ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0e5412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "# print(input[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5924f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
     ]
    }
   ],
   "source": [
    "## Let print attention score\n",
    "query =input[2] # start\n",
    "\n",
    "def atten_score(input,query):\n",
    "    att_score=torch.empty(input.shape[0])\n",
    "    for i,x_i in enumerate(input):\n",
    "        att_score[i] =torch.dot(x_i,query)   ## Simple attention only by dot product \n",
    "    return att_score\n",
    "att_score=atten_score(input,query)\n",
    "print(att_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcfa58",
   "metadata": {},
   "source": [
    "<strong>The main goal behind the normalization</strong> is to obtain attention weights\n",
    "  that sum up to 1.\n",
    "\n",
    "  This normalization is a convention that is useful for interpretation and for\n",
    "  maintaining training stability in an LLM\n",
    "\n",
    "  Here's a straightforward method for achieving this\n",
    "  normalization step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89c59d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1454, 0.2277, 0.2248, 0.1280, 0.1104, 0.1637])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = att_score / att_score.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42b76dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weight after softmax = tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565]) \n"
     ]
    }
   ],
   "source": [
    "## change into softmax attention\n",
    "def softmax_att(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "atten_soft_score=softmax_att(att_score)\n",
    "print(f\"Attention weight after softmax = {atten_soft_score} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a359e51",
   "metadata": {},
   "source": [
    "## Attention Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5412efee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Time taken to run the loop 1.0073270797729492 Sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()  ## Starting time\n",
    "att_table=torch.empty(6,6) ## Create an empty matrix\n",
    "for i,x_i in enumerate(input):\n",
    "    query=input[i]\n",
    "    att_table[i]=atten_score(input,query)\n",
    "time.sleep(1)  ## Increase 1 Sec\n",
    "end = time.time()  ## end time\n",
    "print(att_table)\n",
    "print(f\"Time taken to run the loop {end-start} Sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24d79710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "Time taken to run the loop 1.0011286735534668 Sec\n"
     ]
    }
   ],
   "source": [
    "## Attention Score by using Matrix Multiplication\n",
    "start=time.time()\n",
    "attn_table=input @ input.T\n",
    "time.sleep(1)\n",
    "end=time.time()\n",
    "print(att_table)\n",
    "print(f\"Time taken to run the loop {end-start} Sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399bac4",
   "metadata": {},
   "source": [
    "## Applying direct softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c80ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "## Attention weight matrix\n",
    "attn_weights = torch.softmax(attn_table, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22373897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "## Now find all the context vector\n",
    "all_context_vec=attn_weights @ input\n",
    "print(all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9ea51",
   "metadata": {},
   "source": [
    "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS(Key ,Query,Value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc1db7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "d_in=input.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85e5f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(123)\n",
    "w_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "w_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "w_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed11f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_query Matrix Parameter containing:\n",
      "tensor([[0.4156, 0.0014],\n",
      "        [0.8157, 0.1645],\n",
      "        [0.4039, 0.8877]]) \n",
      "w_key Matrix Parameter containing:\n",
      "tensor([[0.3615, 0.7642],\n",
      "        [0.2305, 0.5352],\n",
      "        [0.1136, 0.5045]]) \n",
      "w_value Matrix Parameter containing:\n",
      "tensor([[0.2898, 0.9282],\n",
      "        [0.8003, 0.9874],\n",
      "        [0.7082, 0.1279]]) \n"
     ]
    }
   ],
   "source": [
    "print(f\"w_query Matrix {w_query} \")\n",
    "print(f\"w_key Matrix {w_key} \")\n",
    "print(f\"w_value Matrix {w_value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "338a0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2049, 0.7297])\n"
     ]
    }
   ],
   "source": [
    "## Let's take the x2 from the and find the corresponding Key ,query and value \n",
    "x_2=input[1]   ## this is for single value x2\n",
    "q_2=x_2 @ w_query\n",
    "v_2=x_2 @ w_key\n",
    "print(q_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1642c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys Matrix: tensor([[0.2911, 0.8579],\n",
      "        [0.4743, 1.2189],\n",
      "        [0.4747, 1.2134],\n",
      "        [0.2507, 0.6451],\n",
      "        [0.3473, 0.7727],\n",
      "        [0.2650, 0.7439]])\n",
      "values Matrix: tensor([[0.8750, 0.6611],\n",
      "        [1.3231, 1.4539],\n",
      "        [1.2987, 1.4502],\n",
      "        [0.7616, 0.8191],\n",
      "        [0.4941, 0.9743],\n",
      "        [1.0442, 0.9067]])\n",
      "queries Matrix: tensor([[0.6606, 0.8153],\n",
      "        [1.2049, 0.7297],\n",
      "        [1.1888, 0.7087],\n",
      "        [0.6978, 0.3887],\n",
      "        [0.5644, 0.1310],\n",
      "        [0.8955, 0.6199]])\n"
     ]
    }
   ],
   "source": [
    "## Now find key , query and value for the all inputs\n",
    "keys=input @ w_key\n",
    "values=input @ w_value\n",
    "queries=input @ w_query\n",
    "\n",
    "print(\"keys Matrix:\", keys)\n",
    "\n",
    "print(\"values Matrix:\", values)\n",
    "\n",
    "print(\"queries Matrix:\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "195e960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4610)\n"
     ]
    }
   ],
   "source": [
    "## Let find the attention score for the x2 dot(q_2,k_2)\n",
    "key_2=keys[1]\n",
    "att_s2=q_2.dot(key_2)\n",
    "print(att_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83bf5c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8917, 1.3071, 1.3029, 0.6915, 0.8594, 0.7815],\n",
      "        [0.9768, 1.4610, 1.4574, 0.7728, 0.9824, 0.8621],\n",
      "        [0.9541, 1.4278, 1.4243, 0.7552, 0.9605, 0.8422],\n",
      "        [0.5366, 0.8048, 0.8029, 0.4257, 0.5427, 0.4740],\n",
      "        [0.2766, 0.4273, 0.4268, 0.2260, 0.2972, 0.2470],\n",
      "        [0.7925, 1.1804, 1.1773, 0.6244, 0.7900, 0.6984]])\n"
     ]
    }
   ],
   "source": [
    "## finding the attention score for the all the iputs\n",
    "attn_scores = queries @ keys.T # omega\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e33b4af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2080, 0.2074, 0.1346, 0.1516, 0.1434],\n",
      "        [0.1514, 0.2132, 0.2127, 0.1311, 0.1520, 0.1396],\n",
      "        [0.1518, 0.2121, 0.2116, 0.1318, 0.1524, 0.1402],\n",
      "        [0.1587, 0.1918, 0.1916, 0.1467, 0.1594, 0.1518],\n",
      "        [0.1617, 0.1799, 0.1798, 0.1560, 0.1641, 0.1584],\n",
      "        [0.1550, 0.2040, 0.2035, 0.1377, 0.1548, 0.1451]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5,dim=-1)   ## divided by sqrt of key dimension\n",
    "print(attn_weights)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c948d",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42e63a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.w_query=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_key=nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_value=nn.Parameter(torch.rand(d_in,d_out))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        query =x @ self.w_query\n",
    "        key=x @ self.w_key\n",
    "        value=x @ self.w_value\n",
    "        \n",
    "        att_score=query @ keys.T \n",
    "        att_weight=torch.softmax(att_score/key.shape[-1]**0.5,dim=-1)\n",
    "        \n",
    "        context_vec=att_weight @ value\n",
    "        \n",
    "        return context_vec\n",
    "\n",
    "# print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e361c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8720, 0.8265],\n",
      "        [0.8921, 0.8483],\n",
      "        [0.8917, 0.8479],\n",
      "        [0.8695, 0.8238],\n",
      "        [0.8707, 0.8250],\n",
      "        [0.8750, 0.8297]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Self_att_cls = SelfAttention(d_in, d_out)\n",
    "print(Self_att_cls(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f885442",
   "metadata": {},
   "source": [
    " ## CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82d71693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2080, 0.2074, 0.1346, 0.1516, 0.1434],\n",
       "        [0.1514, 0.2132, 0.2127, 0.1311, 0.1520, 0.1396],\n",
       "        [0.1518, 0.2121, 0.2116, 0.1318, 0.1524, 0.1402],\n",
       "        [0.1587, 0.1918, 0.1916, 0.1467, 0.1594, 0.1518],\n",
       "        [0.1617, 0.1799, 0.1798, 0.1560, 0.1641, 0.1584],\n",
       "        [0.1550, 0.2040, 0.2035, 0.1377, 0.1548, 0.1451]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67aa03e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK :tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "CAUSAL WEIGHT :tensor([[0.1551, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1514, 0.2132, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1518, 0.2121, 0.2116, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1587, 0.1918, 0.1916, 0.1467, 0.0000, 0.0000],\n",
      "        [0.1617, 0.1799, 0.1798, 0.1560, 0.1641, 0.0000],\n",
      "        [0.1550, 0.2040, 0.2035, 0.1377, 0.1548, 0.1451]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4152, 0.5848, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2637, 0.3686, 0.3677, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2304, 0.2785, 0.2781, 0.2130, 0.0000, 0.0000],\n",
      "        [0.1922, 0.2138, 0.2137, 0.1854, 0.1950, 0.0000],\n",
      "        [0.1550, 0.2040, 0.2035, 0.1377, 0.1548, 0.1451]])\n"
     ]
    }
   ],
   "source": [
    "## applying causal attention\n",
    "mask=torch.tril(torch.ones(attn_weights.shape))\n",
    "print(f\"MASK :{mask}\")\n",
    "causal_weight=mask*attn_weights\n",
    "print(f\"CAUSAL WEIGHT :{causal_weight}\")\n",
    "causal_norm=causal_weight/causal_weight.sum(dim=-1,keepdim=True)\n",
    "print(causal_norm) ## ReNormailazation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526f494",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "- But after re Norm still there are some effect of previous value so , change the 0 element into -inf before applying softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1809b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=torch.triu(torch.ones(attn_weights.shape),diagonal=1)\n",
    "causal_weight=causal_weight.masked_fill(mask.bool(),-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5478ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1514, 0.2132,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1518, 0.2121, 0.2116,   -inf,   -inf,   -inf],\n",
       "        [0.1587, 0.1918, 0.1916, 0.1467,   -inf,   -inf],\n",
       "        [0.1617, 0.1799, 0.1798, 0.1560, 0.1641,   -inf],\n",
       "        [0.1550, 0.2040, 0.2035, 0.1377, 0.1548, 0.1451]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31010803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now apply softmax\n",
    "C_attn_wgt=torch.softmax(causal_weight/keys.shape[-1]**0.5 ,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f54b4368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4891, 0.5109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3240, 0.3381, 0.3380, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2476, 0.2535, 0.2534, 0.2455, 0.0000, 0.0000],\n",
       "        [0.1991, 0.2016, 0.2016, 0.1983, 0.1994, 0.0000],\n",
       "        [0.1653, 0.1711, 0.1710, 0.1633, 0.1652, 0.1641]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_attn_wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b631f",
   "metadata": {},
   "source": [
    "## DROPOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ed31f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "one_tensor = torch.ones(6, 6)\n",
    "print(one_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d92804",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. \n",
    "\n",
    "To compensate for the reduction in active\n",
    "elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/0.5 =2. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights,\n",
    "ensuring that the average influence of the attention mechanism remains consistent during\n",
    "both the training and inference phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "92b83765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 0., 0., 0., 0.],\n",
      "        [2., 0., 0., 2., 0., 0.],\n",
      "        [0., 2., 2., 0., 0., 0.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 0., 0., 2., 2., 0.],\n",
      "        [2., 0., 2., 2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "dropout=torch.nn.Dropout(0.5)\n",
    "print(dropout(one_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "677ace6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3762, -0.3049],\n",
       "         [ 0.4803, -0.4585],\n",
       "         [ 0.5127, -0.5059],\n",
       "         [ 0.4626, -0.4655],\n",
       "         [ 0.4414, -0.4385],\n",
       "         [ 0.4323, -0.4399]],\n",
       "\n",
       "        [[ 0.3762, -0.3049],\n",
       "         [ 0.4803, -0.4585],\n",
       "         [ 0.5127, -0.5059],\n",
       "         [ 0.4626, -0.4655],\n",
       "         [ 0.4414, -0.4385],\n",
       "         [ 0.4323, -0.4399]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias) ## output = x @ W_query.T + b_query\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.dropout = nn.Dropout(dropout) if isinstance(dropout, float) else dropout  # expects a nn.Dropout instance or float\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape   ## B is the number of batches \n",
    "        query = self.w_query(x)\n",
    "        key = self.w_key(x)\n",
    "        value = self.w_value(x)\n",
    "\n",
    "        att_score = query @ key.transpose(1, 2)  #keys.shape = (batch_size, seq_len, d_out) ==>keys.transpose(1, 2).shape = (batch_size, d_out, seq_len)\n",
    "        att_score.masked_fill\n",
    "        att_score = att_score.masked_fill(\n",
    "            self.mask[:num_token, :num_token].bool(), float('-inf')\n",
    "        ) \n",
    "        attn_weights = torch.softmax(\n",
    "            att_score / key.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ value\n",
    "        return context_vec\n",
    "\n",
    "Causal_attn = CausalAttention(3, 2, 6, 0.0)   ## Context_length = Number of token\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)  ## 2 batch\n",
    "Causal_attn(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81bef7",
   "metadata": {},
   "source": [
    "## MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be092c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out=d_out\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_out//num_heads\n",
    "        \n",
    "        self.w_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.w_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key=self.w_key(x)\n",
    "        query=self.w_query(x)\n",
    "        value=self.w_value(x)\n",
    "        \n",
    "            # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys=key.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        queries=query.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        values=value.view(b,num_token,self.num_heads,self.head_dim)\n",
    "        \n",
    "            # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "            \n",
    "        # keys=keys.veiw(b,self.num_heads,num_token,self.head_dim)\n",
    "        # values=values.veiw(b,self.num_heads,num_token,self.head_dim)\n",
    "        # queries=queries.veiw(b,self.num_heads,num_token,self.head_dim)\n",
    "            \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)    ## same as above\n",
    "        \n",
    "        att_score=queries @ keys.transpose(2,3)  # Dot product for each head\n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "        att_score.masked_fill_(mask_bool,-torch.inf)           \n",
    "        att_weight=torch.softmax(att_score/key.shape[-1]**0.5 ,dim=-1)\n",
    "        \n",
    "        att_weight=self.dropout(att_weight)\n",
    "        \n",
    "        context_vec=att_weight @ values \n",
    "        \n",
    "        ## Back to original shape  : (b, num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        context_vec = context_vec.view(b, num_token, self.d_out)\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec) \n",
    "        return context_vec \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a32a2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0866, -0.0584,  0.7947,  0.4610,  0.0820, -0.1711],\n",
      "         [-0.1763, -0.1528,  0.6682,  0.1842,  0.2418,  0.0166],\n",
      "         [ 0.0903, -0.2794,  0.2088,  0.0811,  0.3074,  0.2633]],\n",
      "\n",
      "        [[-0.0866, -0.0584,  0.7947,  0.4610,  0.0820, -0.1711],\n",
      "         [-0.1763, -0.1528,  0.6682,  0.1842,  0.2418,  0.0166],\n",
      "         [ 0.0903, -0.2794,  0.2088,  0.0811,  0.3074,  0.2633]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Apply the above class \n",
    "inputs = torch.tensor(\n",
    "    [[0.67, 0.78, 0.79, 0.70, 0.14, 0.66],  # Row 1\n",
    "     [0.87, 0.34, 0.74, 0.12, 0.54, 0.35],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "input=torch.stack((inputs,inputs),dim=0)\n",
    "d_out=6\n",
    "batch,context_len,d_in=input.shape\n",
    "Muti_head_att=MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "cont_vec = Muti_head_att(input)\n",
    "print(cont_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebc816",
   "metadata": {},
   "source": [
    "## IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd6df2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "443bc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "import tiktoken\n",
    "tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "# text=\"How are you\"\n",
    "# print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ba740",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4645efed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "batch=torch.randn(2,5)\n",
    "layer=nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "output=layer(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc95b6f",
   "metadata": {},
   "source": [
    "Let us apply layer normalization to the layer outputs we obtained earlier. The\n",
    "operation consists of subtracting the mean and dividing by the square root of the variance\n",
    "(also known as standard deviation):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "684b7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=output.mean(dim=-1,keepdim=True)\n",
    "var=output.var(dim=-1,keepdim=True)\n",
    "Norm_out=(output-mean)/torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "154d3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean after Norm :tensor([ 1.9868e-08, -1.4901e-08], grad_fn=<MeanBackward1>)\n",
      "Varinace after Norm :tensor([1.0000, 1.0000], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean after Norm :{Norm_out.mean(dim=-1)}\")   ## Mean are very close to zero\n",
    "print(f\"Varinace after Norm :{Norm_out.var(dim=-1)}\")  ## Variance =1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bdd03",
   "metadata": {},
   "source": [
    "\n",
    "Note that the value 2.9802e-08 in the output tensor is the scientific notation for 2.9802 ×\n",
    "10-8, which is 0.0000000298 in decimal form. This value is very close to 0, but it is not\n",
    "exactly 0 due to small numerical errors that can accumulate because of the finite precision\n",
    "with which computers represent numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83f5820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's Define the LayerNorm Class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,embd_dim):\n",
    "        super().__init__()\n",
    "        self.eps=1e-7\n",
    "        self.scale = nn.Parameter(torch.ones(embd_dim))  ## It is trainable parameter\n",
    "        self.shift = nn.Parameter(torch.zeros(embd_dim)) ## It is also trainable parameter\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True)\n",
    "        norm_x= (x-mean)/torch.sqrt(var+self.eps)\n",
    "        norm_x=norm_x * self.scale +self.shift\n",
    "        return norm_x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fe92f",
   "metadata": {},
   "source": [
    "## FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c6bdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Define \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        result=0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))))\n",
    "        return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba56f3c",
   "metadata": {},
   "source": [
    "Next, let's use the GELU function to implement the small neural network module,\n",
    "FeedForward, that we will be using in the LLM's transformer block later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ee7cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4937dc",
   "metadata": {},
   "source": [
    "## SHORTCUT CONNECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42c3ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            layer_out=layer(x)\n",
    "            ## check is the sahpe are same or not to apply the shortcut\n",
    "            if self.use_shortcut and x.shape== layer_out.shape:\n",
    "                x=x+layer_out\n",
    "            else:\n",
    "                x=layer_out\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdaa98a",
   "metadata": {},
   "source": [
    "Let's use this code to first initialize a neural network without shortcut connections. Here,\n",
    "each layer will be initialized such that it accepts an example with 3 input values and returns\n",
    "3 output values. The last layer returns a single output value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77d6c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size=[3, 3, 3, 3, 2, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shortcut = DeepNeuralNetwork(\n",
    "layer_size, use_shortcut=False\n",
    ")\n",
    "model_with_shortcut=DeepNeuralNetwork(layer_size,use_shortcut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbb5a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight torch.Size([3, 3])\n",
      "layers.0.0.bias torch.Size([3])\n",
      "layers.1.0.weight torch.Size([3, 3])\n",
      "layers.1.0.bias torch.Size([3])\n",
      "layers.2.0.weight torch.Size([3, 3])\n",
      "layers.2.0.bias torch.Size([3])\n",
      "layers.3.0.weight torch.Size([2, 3])\n",
      "layers.3.0.bias torch.Size([2])\n",
      "layers.4.0.weight torch.Size([1, 2])\n",
      "layers.4.0.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, para in model_with_shortcut.named_parameters():\n",
    "\tprint(name, para.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed36004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Without ShortCut tensor([[ 0.2821, -0.1635,  0.2824]], grad_fn=<MulBackward0>) \n",
      "Output With Shortcut tensor([[1.2008, 0.2174, 0.3048]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output Without ShortCut {model_without_shortcut(sample_input)} \")\n",
    "print(f\"Output With Shortcut {model_with_shortcut(sample_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed7f5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            if param.grad is not None:\n",
    "                # Print the mean absolute gradient of the weights\n",
    "                print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "            else:\n",
    "                print(f\"{name} has no gradient (param.grad is None)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5ef9be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.2820483148097992\n",
      "layers.1.0.weight has no gradient (param.grad is None)\n",
      "layers.2.0.weight has no gradient (param.grad is None)\n",
      "layers.3.0.weight has no gradient (param.grad is None)\n",
      "layers.4.0.weight has no gradient (param.grad is None)\n"
     ]
    }
   ],
   "source": [
    "check_grad(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2d42942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.3525603711605072\n",
      "layers.1.0.weight has no gradient (param.grad is None)\n",
      "layers.2.0.weight has no gradient (param.grad is None)\n",
      "layers.3.0.weight has no gradient (param.grad is None)\n",
      "layers.4.0.weight has no gradient (param.grad is None)\n",
      "Output Without ShortCut Gradient :None \n",
      "layers.0.0.weight has gradient mean of 0.20850218832492828\n",
      "layers.1.0.weight has no gradient (param.grad is None)\n",
      "layers.2.0.weight has no gradient (param.grad is None)\n",
      "layers.3.0.weight has no gradient (param.grad is None)\n",
      "layers.4.0.weight has no gradient (param.grad is None)\n",
      "Output With Shortcut Gradient:  None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output Without ShortCut Gradient :{check_grad(model_without_shortcut, sample_input)} \")\n",
    "print(f\"Output With Shortcut Gradient:  {check_grad(model_with_shortcut, sample_input)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcc300",
   "metadata": {},
   "source": [
    "##  ATTENTION AND LINEAR LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0bddec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "   \n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62167443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) #A\n",
    "block = Transformer(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d081c",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77246fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb=nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
    "        self.pos_emb=nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
    "        self.drop_emb=nn.Dropout(cfg[\"drop_rate\"] )\n",
    "        \n",
    "        self.trf_block=nn.Sequential(\n",
    "            *[Transformer(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm=LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head=nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self,in_idx):\n",
    "        batch_size,seq_len=in_idx.shape  ## INput index\n",
    "        tok_emb = self.tok_emb(in_idx)   ## Convert the ids into embedding spaces\n",
    "        # pos_emb= self.pos_emb(in_idx)\n",
    "        device = in_idx.device  # to keep it on the same device (CPU or GPU)\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0)  # shape: (1, seq_len)\n",
    "        pos_emb = self.pos_emb(positions)  # shape: (1, seq_len, emb_dim)\n",
    "\n",
    "        x=tok_emb + pos_emb\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits      \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a146930",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=torch. tensor([[610, 3621,60,  3451],\n",
    "        [610, 111, 622,  257]])\n",
    "\n",
    "model=GPTModel(GPT_CONFIG_124M)\n",
    "out=model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f29b68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2927,  0.4366,  0.5312,  ..., -0.5746,  0.6668,  0.1098],\n",
       "         [ 1.5808,  1.3435,  0.5079,  ..., -0.2106,  0.0256, -0.0559],\n",
       "         [-0.0593,  0.1581, -0.7858,  ...,  0.1746,  0.1069,  0.0903],\n",
       "         [ 0.4270,  1.4716,  0.1388,  ..., -0.1685, -0.3926,  0.2356]],\n",
       "\n",
       "        [[-0.0949,  0.4965,  0.7375,  ..., -0.7842,  0.6145,  0.4029],\n",
       "         [ 0.0151,  1.0135, -0.5840,  ..., -0.4165, -0.4926, -0.3334],\n",
       "         [ 0.9529,  0.0164, -0.4429,  ...,  0.0536,  0.6197,  0.5303],\n",
       "         [ 1.3596,  0.8030,  0.0059,  ...,  0.1113, -0.1000, -0.0966]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2ab235ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3621) 50257\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(batch), GPT_CONFIG_124M[\"vocab_size\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e043d71",
   "metadata": {},
   "source": [
    "## GENERATING TEXT FROM OUTPUT TOKENS\n",
    "Step 1: idx is a (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "Step 2: Crop current context if it exceeds the supported context size E.g., if LLM supports only 5 tokens, and the\n",
    "context size is 10 then only the last 5 tokens are used as context\n",
    "\n",
    "Step 3: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "\n",
    "Step 4: probas has shape (batch, vocab_size)\n",
    "\n",
    "Step 5: idx_next has shape (batch, 1)\n",
    "\n",
    "Step 6: Append sampled index to the running sequence, where idx has shape (batch, n_tokens+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e8f75155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16ce9a",
   "metadata": {},
   "source": [
    "## Give a Input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4ed0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c5661",
   "metadata": {},
   "source": [
    "## Produce output ID'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ecbd0d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 43400, 34877,  4139, 19528, 12083, 26108]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M\n",
    "[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c282a",
   "metadata": {},
   "source": [
    "## Decode ID to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a95e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I ambleretsy Minister backwardding Regulations\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
